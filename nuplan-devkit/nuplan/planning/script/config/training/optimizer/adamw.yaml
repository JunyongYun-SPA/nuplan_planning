_target_: torch.optim.AdamW
_convert_: 'all'

lr: 5e-4  # learning rate 1e-4
weight_decay: 1e-4  # weight decay coefficient 1e-5
betas: [0.9, 0.999]  # coefficients used for computing running averages of gradient and its square
